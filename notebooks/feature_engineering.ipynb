{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path.home() / 'work' / 'data'\n",
    "DATA_RAW = DATA / 'raw'\n",
    "DATA_PROCESSED = DATA / 'processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_truncated = pd.read_parquet(DATA_PROCESSED / 'train_ts_truncated.parquet')\n",
    "test = pd.read_parquet(DATA_PROCESSED / 'test_ts_truncated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_parquet(DATA_PROCESSED / 'train_target.parquet')\n",
    "test_target = pd.read_parquet(DATA_PROCESSED / 'test_target.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_parquet(DATA_RAW / 'test_values.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_metadata = pd.read_csv(DATA_RAW / 'recipe_metadata.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DATA_RAW / 'submission_format.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['tank_lsh_acid', 'tank_lsh_pre_rinse', 'target_time_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_truncated = train_truncated.drop(to_drop, axis=1)\n",
    "test = test.drop(to_drop, axis=1)\n",
    "final_test = final_test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = ['object_id', 'pipeline']\n",
    "phase_order = ['pre_rinse', 'caustic', 'intermediate_rinse', 'acid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_count(timeseries):\n",
    "    df = timeseries.groupby('process_id')['phase'].unique()\n",
    "    df = df.apply('|'.join).str.get_dummies().astype(bool)\n",
    "    df = df.reindex(phase_order, axis=1)\n",
    "    df.columns = 'phase:' + df.columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_duration(timeseries):\n",
    "    df = timeseries.groupby(['process_id', 'phase'])['timestamp'].agg(['min', 'max'])\n",
    "    df['duration'] = (df['max'] - df['min']).dt.total_seconds()\n",
    "    df = df[['duration']].unstack(-1)\n",
    "    df.columns = df.columns.to_series().apply(lambda g: ':'.join(g[::-1]))  # phase first in the name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_turbidity(timeseries):\n",
    "    \"\"\"Calculates the target value for all phases (for the entire duration).\"\"\"\n",
    "    df = timeseries.groupby(['process_id', 'phase'])['turbidity'].sum().unstack()\n",
    "    df.columns = df.columns.to_series() + ':total_turbidity'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_and_last(data, groupby_cols, float_cols, N):\n",
    "    first = data[groupby_cols + float_cols].groupby(groupby_cols).head(N).groupby(groupby_cols).mean()\n",
    "    last  = data[groupby_cols + float_cols].groupby(groupby_cols).tail(N).groupby(groupby_cols).mean()\n",
    "    first.columns = pd.MultiIndex.from_tuples([(col, 'first%d' % N) for col in first.columns])\n",
    "    last.columns  = pd.MultiIndex.from_tuples([(col,  'last%d' % N) for col in last.columns])\n",
    "    return pd.concat([first, last], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_funcs(data, groupby_cols=['process_id']):\n",
    "    \"\"\"Faster than aggregating all functions at once. The hard part is recreating the multi-index columns.\"\"\"\n",
    "    float_cols = data.select_dtypes('float').columns.tolist()\n",
    "    \n",
    "    # agg standard functions\n",
    "    standard = data.groupby(groupby_cols)[float_cols].agg(['sum', 'median', 'mean', 'std', 'max', 'min'])\n",
    "\n",
    "    # apply quantile\n",
    "    qs = [.2, .8]\n",
    "    quantiles = data.groupby(groupby_cols)[float_cols].quantile(qs).unstack(-1)\n",
    "    quantiles.columns = pd.MultiIndex.from_tuples([(col[0], 'q%d' % int(100 * col[1])) for col in quantiles.columns.get_values()])\n",
    "\n",
    "    # calculate average of first and last values\n",
    "    first_and_last = get_first_and_last(data, groupby_cols, float_cols, 10)  # last param somewhat optimized\n",
    "    \n",
    "    return pd.concat([standard, quantiles, first_and_last], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_and_apply(timeseries, prefix=''):\n",
    "    df1 = apply_funcs(timeseries)\n",
    "    df1.columns = prefix + df1.columns.to_series().apply(':'.join)\n",
    "    \n",
    "    df2 = apply_funcs(timeseries, ['process_id', 'phase'])\n",
    "    df2 = df2.unstack(-1)  # make phase a third index level of columns\n",
    "    df2.columns = prefix + df2.columns.to_series().apply(':'.join)\n",
    "    \n",
    "    return pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_features_fast(timeseries, add_diff=True):\n",
    "    \"\"\"Select features of the timeseries, its derivative, all grouped by process or process and phase.\"\"\"\n",
    "    df1 = groupby_and_apply(timeseries)\n",
    "    \n",
    "    if add_diff:\n",
    "        float_cols = timeseries.select_dtypes('float').columns.tolist()\n",
    "        # diff = timeseries[float_cols].diff().where(timeseries['process_id'].shift() == timeseries['process_id'], np.nan)\n",
    "        smoothing_window = 10\n",
    "        diff = timeseries.groupby('process_id')[float_cols].transform(\n",
    "            lambda g: g.rolling(smoothing_window).mean().diff(smoothing_window))\n",
    "        \n",
    "        df2 = groupby_and_apply(pd.concat([timeseries[['process_id', 'phase']], diff],       axis=1).dropna(), prefix='diff:')\n",
    "        df3 = groupby_and_apply(pd.concat([timeseries[['process_id', 'phase']], diff.abs()], axis=1).dropna(), prefix='absdiff:')\n",
    "        \n",
    "        return pd.concat([df1, df2, df3], axis=1)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_features_fast(timeseries):\n",
    "    bool_columns = timeseries.select_dtypes('bool').columns.tolist()\n",
    "    data = timeseries[['process_id', 'phase'] + bool_columns].copy()\n",
    "    data[bool_columns] = data[bool_columns].astype(int)\n",
    "    \n",
    "    # percentage of time the boolean value is \"on\" for each phase...\n",
    "    df = data.groupby(['process_id', 'phase'])[bool_columns].agg(['sum', 'mean'])\n",
    "    df = df.unstack(-1)\n",
    "    df.columns = df.columns.to_series().apply(':'.join)\n",
    "    # ... and in total\n",
    "    df1 = data.groupby('process_id')[bool_columns].agg(['sum', 'mean'])\n",
    "    df1.columns = df1.columns.to_series().apply(':'.join)\n",
    "    \n",
    "    # first and last values\n",
    "    fl = get_first_and_last(data, ['process_id', 'phase'], bool_columns, 10)\n",
    "    fl = fl.unstack(-1)\n",
    "    fl.columns = fl.columns.to_series().apply(':'.join)\n",
    "\n",
    "    fl1 = get_first_and_last(data, ['process_id'], bool_columns, 10)\n",
    "    fl1.columns = fl1.columns.to_series().apply(':'.join)\n",
    "\n",
    "    return pd.concat([df, df1, fl, fl1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_features(timeseries):\n",
    "    \"\"\"Calculates three maximums of the power spectrum.\"\"\"\n",
    "    def power_spectrum(t):\n",
    "        A = np.fft.fft(t)  # Discrete Fourier Transform\n",
    "        return np.abs(A) ** 2\n",
    "    \n",
    "    df = timeseries.groupby('process_id')['turbidity'].apply(lambda x: np.sort(power_spectrum(x))[-3:]).apply(pd.Series)\n",
    "    df.columns = ['harmonic:power_spectrum_max{}'.format(i) for i in [3, 2, 1]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(timeseries):\n",
    "    features = timeseries.groupby('process_id')[metadata].first().astype(str)\n",
    "    features = features.join(recipe_metadata.astype(str).apply(''.join, axis=1).rename('recipe'))\n",
    "    if features.isna().any().any():\n",
    "        raise ValueError('Missing values for %s' % str(metadata))\n",
    "        \n",
    "    timeseries['log_return_turbidity'] = np.log(1 + timeseries['return_turbidity'])\n",
    "    timeseries['turbidity'] = np.maximum(timeseries['return_flow'], 0) * timeseries['return_turbidity']\n",
    "    timeseries['log_turbidity'] = np.log(1 + timeseries['turbidity'].clip(lower=0))\n",
    "    \n",
    "    print('Adding boolean features...')\n",
    "    features = features.join(boolean_features_fast(timeseries))\n",
    "    print('Adding phase count...')\n",
    "    features = features.join(phase_count(timeseries))\n",
    "    print('Adding phase duration...')\n",
    "    features = features.join(phase_duration(timeseries))\n",
    "    print('Adding total turbidity...')\n",
    "    features = features.join(total_turbidity(timeseries))\n",
    "    print('Adding float features (this may take a while)...')\n",
    "    features = features.join(float_features_fast(timeseries, add_diff=False))\n",
    "    print('Adding harmonic features...')\n",
    "    features = features.join(harmonic_features(timeseries))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding boolean features...\n",
      "Adding phase count...\n",
      "Adding phase duration...\n",
      "Adding total turbidity...\n",
      "Adding float features (this may take a while)...\n",
      "Adding harmonic features...\n",
      "CPU times: user 16.1 s, sys: 2.2 s, total: 18.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_features = join_features(train_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding boolean features...\n",
      "Adding phase count...\n",
      "Adding phase duration...\n",
      "Adding total turbidity...\n",
      "Adding float features (this may take a while)...\n",
      "Adding harmonic features...\n",
      "CPU times: user 7.15 s, sys: 827 ms, total: 7.98 s\n",
      "Wall time: 7.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_features = join_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding boolean features...\n",
      "Adding phase count...\n",
      "Adding phase duration...\n",
      "Adding total turbidity...\n",
      "Adding float features (this may take a while)...\n",
      "Adding harmonic features...\n",
      "CPU times: user 13.5 s, sys: 1.61 s, total: 15.2 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_test_features = join_features(final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (train_features.index == train_target.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (test_features.index == test_target.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (final_test_features.index == submission_format.index).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_parquet(DATA_PROCESSED / 'train_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.to_parquet(DATA_PROCESSED / 'test_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_features.to_parquet(DATA_PROCESSED / 'final_test_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
